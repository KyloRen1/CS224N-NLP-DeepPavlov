{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "boolq-kernel",
   "language": "python",
   "display_name": "boolq-kernel"
  },
  "colab": {
   "name": "debug.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "a9060ff53da1494ebdb7ffe4f1d26ee2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_4c5c8877cf7143bba35a89225c734968",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_e8f1af64591449f2b6de39f09ca770aa",
       "IPY_MODEL_46f021ef8a244bd4bbf3edd90faf20e1"
      ]
     }
    },
    "4c5c8877cf7143bba35a89225c734968": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": "row wrap",
      "width": "100%",
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": "inline-flex",
      "left": null
     }
    },
    "e8f1af64591449f2b6de39f09ca770aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_42d3c2a8a6f7484fb740a8cddb924d7a",
      "_dom_classes": [],
      "description": "Validation sanity check: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "info",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_bc1e52db115c4284a08a8dc8a557fc78"
     }
    },
    "46f021ef8a244bd4bbf3edd90faf20e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_f221ae08113d471784077c9a053b244a",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 2/2 [00:03&lt;00:00,  1.94s/it]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_af79272a43144743abddb673e5b5b2ac"
     }
    },
    "42d3c2a8a6f7484fb740a8cddb924d7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "bc1e52db115c4284a08a8dc8a557fc78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": "2",
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "f221ae08113d471784077c9a053b244a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "af79272a43144743abddb673e5b5b2ac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "bf98698817e94bb696192df37afd27e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_41edde35edb34fe480f21b64182ed3eb",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_b3b5ba46b4974bc3a2a27b9dce201ef1",
       "IPY_MODEL_705d8386405b42298121c6833994ee76"
      ]
     }
    },
    "41edde35edb34fe480f21b64182ed3eb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": "row wrap",
      "width": "100%",
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": "inline-flex",
      "left": null
     }
    },
    "b3b5ba46b4974bc3a2a27b9dce201ef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_ee1df4dfabea415a9f54d70e58dab35d",
      "_dom_classes": [],
      "description": "Epoch 1:   9%",
      "_model_name": "FloatProgressModel",
      "bar_style": "info",
      "max": 223,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 21,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_d10efd4b78f74785a0f564b22db68f19"
     }
    },
    "705d8386405b42298121c6833994ee76": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_fadd6cd3fcfc4645b9d9e4952c2846a1",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 21/223 [00:14&lt;02:16,  1.48it/s, loss=0.661, v_num=25zz8sf7]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_ea8d732850a5412395dc40ba54e14c72"
     }
    },
    "ee1df4dfabea415a9f54d70e58dab35d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "d10efd4b78f74785a0f564b22db68f19": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": "2",
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "fadd6cd3fcfc4645b9d9e4952c2846a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "ea8d732850a5412395dc40ba54e14c72": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "aQkDpj64UOZX",
    "colab_type": "code",
    "outputId": "003925ea-0e57-431b-db5f-37395b25ef4d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install pytorch_lightning simplejson transformers wandb torch"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.6/dist-packages (0.7.6)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.6/dist-packages (3.17.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.8.36)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.18.4)\n",
      "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (3.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.14.4)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.2)\n",
      "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.9.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (46.1.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.10.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.2.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.34.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.6.0.post3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.28.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "fR-lH0LFTzZv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import simplejson as json\n",
    "import wandb\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import BertTokenizer,\\\n",
    "                         BertModel, \\\n",
    "                         BertForSequenceClassification\n",
    "from torchtext import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from pytorch_lightning.logging import WandbLogger\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "DATA = Path(\"../data/\")\n",
    "DATA = Path(\"/content/drive/My Drive/boolq/data\")\n",
    "config = {\n",
    "    \"device\": \"cuda\",\n",
    "    \"bert_pretrained\": \"bert-base-multilingual-cased\",\n",
    "    \"batch_size\": 48,\n",
    "    \"start_lr\": 1e-4,\n",
    "    \"lr_factor\": 0.3\n",
    "    \n",
    "}"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sCu5VSB7rYMi",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0e921058-df77-4849-cf92-ca6462bb1fc8"
   },
   "source": [
    "# !wandb login"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter: Aborted!\n",
      "^C\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3VtC4xhYTzZ1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_prepared_dataset(\n",
    "        raw_data: str,\n",
    "        tokenizer: BertTokenizer,\n",
    "        device: str) -> data.Dataset:\n",
    "\n",
    "    input_idxs = data.Field(\n",
    "        use_vocab=False,\n",
    "        pad_token=tokenizer.pad_token_id,\n",
    "        batch_first=True\n",
    "    )\n",
    "    type_idxs = data.Field(\n",
    "        use_vocab=False,\n",
    "        pad_token=1,\n",
    "        batch_first=True\n",
    "    )\n",
    "\n",
    "    answers = data.Field(\n",
    "        use_vocab=False,\n",
    "        sequential=False,\n",
    "        batch_first=True,\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "\n",
    "    fields = [\n",
    "        (\"x\", input_idxs),\n",
    "        (\"type_ids\", type_idxs),\n",
    "        (\"ans\", answers)\n",
    "    ]\n",
    "\n",
    "    examples = []\n",
    "    with open(raw_data) as f:\n",
    "        for item in f:\n",
    "            item = json.loads(item)\n",
    "            tokenized = tokenizer.encode_plus(\n",
    "                item[\"question\"], item[\"passage\"],\n",
    "                return_token_type_ids=True,\n",
    "                add_special_tokens=True,\n",
    "                max_length=512,\n",
    "            )\n",
    "            examples.append(\n",
    "                data.Example.fromlist(\n",
    "                    (tokenized['input_ids'],\n",
    "                     tokenized['token_type_ids'],\n",
    "                     int(item[\"answer\"])\n",
    "                     ), fields)\n",
    "            )\n",
    "\n",
    "    return data.Dataset(examples, fields)\n",
    "\n",
    "\n",
    "class BoolqClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            config[\"bert_pretrained\"]).to(config[\"device\"])\n",
    "        self.bert.requires_grad_(False)\n",
    "        self.bert.eval()\n",
    "        self.classifier = torch.nn.Linear(768, 1)\n",
    "        self.bert.encoder.layer[-1].requires_grad_(True)\n",
    "        # self.bert.pooler.requires_grad_(True)\n",
    "\n",
    "    def forward(self, x, type_ids):\n",
    "        x = self.bert(x, token_type_ids=type_ids)[0][:, 0]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        y_pred = F.sigmoid(self(batch.x, batch.type_ids).view(-1))\n",
    "        loss = F.binary_cross_entropy(y_pred, batch.ans)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        logs = {'avg_loss': avg_loss}\n",
    "        return {'t_loss': avg_loss, 'log': logs}\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        y_pred = F.sigmoid(self(batch.x, batch.type_ids).view(-1))\n",
    "        loss = F.binary_cross_entropy(y_pred, batch.ans)\n",
    "        acc = accuracy_score(batch.ans.cpu(), y_pred.cpu() > 0.5)\n",
    "        logs = {'val_loss': loss, \"val_acc\": acc}\n",
    "        return {'val_loss': loss, \"val_acc\": acc, 'log': logs}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = np.mean([x['val_acc'] for x in outputs])\n",
    "        logs = {'avg_val_loss': avg_loss, \"avg_val_acc\": avg_acc}\n",
    "        lr = self._get_lr(self.trainer.optimizers[0])\n",
    "        return {'avg_val_loss': avg_loss, \"val_acc\": avg_acc, \"lr\": lr, 'log': logs}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), config[\"start_lr\"])\n",
    "        sch = {\n",
    "          'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "              opt,\n",
    "              factor=config[\"lr_factor\"],\n",
    "              patience=1),\n",
    "          'monitor': 'avg_val_loss',\n",
    "          'interval': 'epoch',\n",
    "          'frequency': 1\n",
    "        }\n",
    "        return [opt], [sch]\n",
    "               \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_lr(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P8VBDmhxTzZ6",
    "colab_type": "code",
    "outputId": "e70eca29-1aa9-457f-d694-2c19cba41d66",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    config[\"bert_pretrained\"], cased=True)\n",
    "\n",
    "train_dataset = get_prepared_dataset(\n",
    "    DATA.joinpath(\"train.jsonl\"),\n",
    "    tokenizer,\n",
    "    config[\"device\"])\n",
    "\n",
    "train_loader = data.BucketIterator(\n",
    "    train_dataset,\n",
    "    config[\"batch_size\"],\n",
    "    device=config[\"device\"],\n",
    "    sort_key=lambda x: len(x.x),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = get_prepared_dataset(\n",
    "    DATA.joinpath(\"dev.jsonl\"),\n",
    "    tokenizer,\n",
    "    config[\"device\"])\n",
    "\n",
    "val_loader = data.BucketIterator(\n",
    "    val_dataset,\n",
    "    128,\n",
    "    device=config[\"device\"],\n",
    "    sort_key=lambda x: len(x.x),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    len(val_loader),\n",
    "    len(train_loader)\n",
    ")"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "26 197\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GZEGONhZTzaA",
    "colab_type": "code",
    "outputId": "4e4b2699-8f62-46b2-e519-b7bb69ef6024",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    }
   },
   "source": [
    "model = BoolqClassifier(config)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)\n",
    "\n",
    "logger = WandbLogger(\n",
    "    offline=False,\n",
    "    name=\"bert_baseline_last_layer_classifier\",\n",
    "    project=\"boolq\",\n",
    "    entity=\"morgachev\",\n",
    ")\n",
    "logger.log_hyperparams(config)\n",
    "# logger.watch(model, log_freq=6)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/morgachev/boolq\" target=\"_blank\">https://app.wandb.ai/morgachev/boolq</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/morgachev/boolq/runs/25zz8sf7\" target=\"_blank\">https://app.wandb.ai/morgachev/boolq/runs/25zz8sf7</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "G74x_FToTzaF",
    "colab_type": "code",
    "outputId": "515fc586-91e8-4488-98fc-a8e37a9d4a11",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a9060ff53da1494ebdb7ffe4f1d26ee2",
      "4c5c8877cf7143bba35a89225c734968",
      "e8f1af64591449f2b6de39f09ca770aa",
      "46f021ef8a244bd4bbf3edd90faf20e1",
      "42d3c2a8a6f7484fb740a8cddb924d7a",
      "bc1e52db115c4284a08a8dc8a557fc78",
      "f221ae08113d471784077c9a053b244a",
      "af79272a43144743abddb673e5b5b2ac",
      "bf98698817e94bb696192df37afd27e8",
      "41edde35edb34fe480f21b64182ed3eb",
      "b3b5ba46b4974bc3a2a27b9dce201ef1",
      "705d8386405b42298121c6833994ee76",
      "ee1df4dfabea415a9f54d70e58dab35d",
      "d10efd4b78f74785a0f564b22db68f19",
      "fadd6cd3fcfc4645b9d9e4952c2846a1",
      "ea8d732850a5412395dc40ba54e14c72"
     ]
    }
   },
   "source": [
    "early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='avg_val_loss',\n",
    "    min_delta=0.00,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    gpus=1,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    # fast_dev_run=True,\n",
    "#     overfit_pct=0.1,\n",
    "#     train_percent_check=0.1,\n",
    "#     val_percent_check=0.1,\n",
    "#     test_percent_check=0.1,\n",
    "#     val_check_interval=0.1\n",
    "#     auto_lr_find=True,\n",
    "#     row_log_interval=10\n",
    " )\n",
    "trainer.fit(model)\n",
    "wandb.save(\"debug.ipynb\")"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "    | Name                                             | Type              | Params\n",
      "-----------------------------------------------------------------------------------\n",
      "0   | bert                                             | BertModel         | 177 M \n",
      "1   | bert.embeddings                                  | BertEmbeddings    | 92 M  \n",
      "2   | bert.embeddings.word_embeddings                  | Embedding         | 91 M  \n",
      "3   | bert.embeddings.position_embeddings              | Embedding         | 393 K \n",
      "4   | bert.embeddings.token_type_embeddings            | Embedding         | 1 K   \n",
      "5   | bert.embeddings.LayerNorm                        | LayerNorm         | 1 K   \n",
      "6   | bert.embeddings.dropout                          | Dropout           | 0     \n",
      "7   | bert.encoder                                     | BertEncoder       | 85 M  \n",
      "8   | bert.encoder.layer                               | ModuleList        | 85 M  \n",
      "9   | bert.encoder.layer.0                             | BertLayer         | 7 M   \n",
      "10  | bert.encoder.layer.0.attention                   | BertAttention     | 2 M   \n",
      "11  | bert.encoder.layer.0.attention.self              | BertSelfAttention | 1 M   \n",
      "12  | bert.encoder.layer.0.attention.self.query        | Linear            | 590 K \n",
      "13  | bert.encoder.layer.0.attention.self.key          | Linear            | 590 K \n",
      "14  | bert.encoder.layer.0.attention.self.value        | Linear            | 590 K \n",
      "15  | bert.encoder.layer.0.attention.self.dropout      | Dropout           | 0     \n",
      "16  | bert.encoder.layer.0.attention.output            | BertSelfOutput    | 592 K \n",
      "17  | bert.encoder.layer.0.attention.output.dense      | Linear            | 590 K \n",
      "18  | bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "19  | bert.encoder.layer.0.attention.output.dropout    | Dropout           | 0     \n",
      "20  | bert.encoder.layer.0.intermediate                | BertIntermediate  | 2 M   \n",
      "21  | bert.encoder.layer.0.intermediate.dense          | Linear            | 2 M   \n",
      "22  | bert.encoder.layer.0.output                      | BertOutput        | 2 M   \n",
      "23  | bert.encoder.layer.0.output.dense                | Linear            | 2 M   \n",
      "24  | bert.encoder.layer.0.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "25  | bert.encoder.layer.0.output.dropout              | Dropout           | 0     \n",
      "26  | bert.encoder.layer.1                             | BertLayer         | 7 M   \n",
      "27  | bert.encoder.layer.1.attention                   | BertAttention     | 2 M   \n",
      "28  | bert.encoder.layer.1.attention.self              | BertSelfAttention | 1 M   \n",
      "29  | bert.encoder.layer.1.attention.self.query        | Linear            | 590 K \n",
      "30  | bert.encoder.layer.1.attention.self.key          | Linear            | 590 K \n",
      "31  | bert.encoder.layer.1.attention.self.value        | Linear            | 590 K \n",
      "32  | bert.encoder.layer.1.attention.self.dropout      | Dropout           | 0     \n",
      "33  | bert.encoder.layer.1.attention.output            | BertSelfOutput    | 592 K \n",
      "34  | bert.encoder.layer.1.attention.output.dense      | Linear            | 590 K \n",
      "35  | bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "36  | bert.encoder.layer.1.attention.output.dropout    | Dropout           | 0     \n",
      "37  | bert.encoder.layer.1.intermediate                | BertIntermediate  | 2 M   \n",
      "38  | bert.encoder.layer.1.intermediate.dense          | Linear            | 2 M   \n",
      "39  | bert.encoder.layer.1.output                      | BertOutput        | 2 M   \n",
      "40  | bert.encoder.layer.1.output.dense                | Linear            | 2 M   \n",
      "41  | bert.encoder.layer.1.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "42  | bert.encoder.layer.1.output.dropout              | Dropout           | 0     \n",
      "43  | bert.encoder.layer.2                             | BertLayer         | 7 M   \n",
      "44  | bert.encoder.layer.2.attention                   | BertAttention     | 2 M   \n",
      "45  | bert.encoder.layer.2.attention.self              | BertSelfAttention | 1 M   \n",
      "46  | bert.encoder.layer.2.attention.self.query        | Linear            | 590 K \n",
      "47  | bert.encoder.layer.2.attention.self.key          | Linear            | 590 K \n",
      "48  | bert.encoder.layer.2.attention.self.value        | Linear            | 590 K \n",
      "49  | bert.encoder.layer.2.attention.self.dropout      | Dropout           | 0     \n",
      "50  | bert.encoder.layer.2.attention.output            | BertSelfOutput    | 592 K \n",
      "51  | bert.encoder.layer.2.attention.output.dense      | Linear            | 590 K \n",
      "52  | bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "53  | bert.encoder.layer.2.attention.output.dropout    | Dropout           | 0     \n",
      "54  | bert.encoder.layer.2.intermediate                | BertIntermediate  | 2 M   \n",
      "55  | bert.encoder.layer.2.intermediate.dense          | Linear            | 2 M   \n",
      "56  | bert.encoder.layer.2.output                      | BertOutput        | 2 M   \n",
      "57  | bert.encoder.layer.2.output.dense                | Linear            | 2 M   \n",
      "58  | bert.encoder.layer.2.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "59  | bert.encoder.layer.2.output.dropout              | Dropout           | 0     \n",
      "60  | bert.encoder.layer.3                             | BertLayer         | 7 M   \n",
      "61  | bert.encoder.layer.3.attention                   | BertAttention     | 2 M   \n",
      "62  | bert.encoder.layer.3.attention.self              | BertSelfAttention | 1 M   \n",
      "63  | bert.encoder.layer.3.attention.self.query        | Linear            | 590 K \n",
      "64  | bert.encoder.layer.3.attention.self.key          | Linear            | 590 K \n",
      "65  | bert.encoder.layer.3.attention.self.value        | Linear            | 590 K \n",
      "66  | bert.encoder.layer.3.attention.self.dropout      | Dropout           | 0     \n",
      "67  | bert.encoder.layer.3.attention.output            | BertSelfOutput    | 592 K \n",
      "68  | bert.encoder.layer.3.attention.output.dense      | Linear            | 590 K \n",
      "69  | bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "70  | bert.encoder.layer.3.attention.output.dropout    | Dropout           | 0     \n",
      "71  | bert.encoder.layer.3.intermediate                | BertIntermediate  | 2 M   \n",
      "72  | bert.encoder.layer.3.intermediate.dense          | Linear            | 2 M   \n",
      "73  | bert.encoder.layer.3.output                      | BertOutput        | 2 M   \n",
      "74  | bert.encoder.layer.3.output.dense                | Linear            | 2 M   \n",
      "75  | bert.encoder.layer.3.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "76  | bert.encoder.layer.3.output.dropout              | Dropout           | 0     \n",
      "77  | bert.encoder.layer.4                             | BertLayer         | 7 M   \n",
      "78  | bert.encoder.layer.4.attention                   | BertAttention     | 2 M   \n",
      "79  | bert.encoder.layer.4.attention.self              | BertSelfAttention | 1 M   \n",
      "80  | bert.encoder.layer.4.attention.self.query        | Linear            | 590 K \n",
      "81  | bert.encoder.layer.4.attention.self.key          | Linear            | 590 K \n",
      "82  | bert.encoder.layer.4.attention.self.value        | Linear            | 590 K \n",
      "83  | bert.encoder.layer.4.attention.self.dropout      | Dropout           | 0     \n",
      "84  | bert.encoder.layer.4.attention.output            | BertSelfOutput    | 592 K \n",
      "85  | bert.encoder.layer.4.attention.output.dense      | Linear            | 590 K \n",
      "86  | bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "87  | bert.encoder.layer.4.attention.output.dropout    | Dropout           | 0     \n",
      "88  | bert.encoder.layer.4.intermediate                | BertIntermediate  | 2 M   \n",
      "89  | bert.encoder.layer.4.intermediate.dense          | Linear            | 2 M   \n",
      "90  | bert.encoder.layer.4.output                      | BertOutput        | 2 M   \n",
      "91  | bert.encoder.layer.4.output.dense                | Linear            | 2 M   \n",
      "92  | bert.encoder.layer.4.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "93  | bert.encoder.layer.4.output.dropout              | Dropout           | 0     \n",
      "94  | bert.encoder.layer.5                             | BertLayer         | 7 M   \n",
      "95  | bert.encoder.layer.5.attention                   | BertAttention     | 2 M   \n",
      "96  | bert.encoder.layer.5.attention.self              | BertSelfAttention | 1 M   \n",
      "97  | bert.encoder.layer.5.attention.self.query        | Linear            | 590 K \n",
      "98  | bert.encoder.layer.5.attention.self.key          | Linear            | 590 K \n",
      "99  | bert.encoder.layer.5.attention.self.value        | Linear            | 590 K \n",
      "100 | bert.encoder.layer.5.attention.self.dropout      | Dropout           | 0     \n",
      "101 | bert.encoder.layer.5.attention.output            | BertSelfOutput    | 592 K \n",
      "102 | bert.encoder.layer.5.attention.output.dense      | Linear            | 590 K \n",
      "103 | bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "104 | bert.encoder.layer.5.attention.output.dropout    | Dropout           | 0     \n",
      "105 | bert.encoder.layer.5.intermediate                | BertIntermediate  | 2 M   \n",
      "106 | bert.encoder.layer.5.intermediate.dense          | Linear            | 2 M   \n",
      "107 | bert.encoder.layer.5.output                      | BertOutput        | 2 M   \n",
      "108 | bert.encoder.layer.5.output.dense                | Linear            | 2 M   \n",
      "109 | bert.encoder.layer.5.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "110 | bert.encoder.layer.5.output.dropout              | Dropout           | 0     \n",
      "111 | bert.encoder.layer.6                             | BertLayer         | 7 M   \n",
      "112 | bert.encoder.layer.6.attention                   | BertAttention     | 2 M   \n",
      "113 | bert.encoder.layer.6.attention.self              | BertSelfAttention | 1 M   \n",
      "114 | bert.encoder.layer.6.attention.self.query        | Linear            | 590 K \n",
      "115 | bert.encoder.layer.6.attention.self.key          | Linear            | 590 K \n",
      "116 | bert.encoder.layer.6.attention.self.value        | Linear            | 590 K \n",
      "117 | bert.encoder.layer.6.attention.self.dropout      | Dropout           | 0     \n",
      "118 | bert.encoder.layer.6.attention.output            | BertSelfOutput    | 592 K \n",
      "119 | bert.encoder.layer.6.attention.output.dense      | Linear            | 590 K \n",
      "120 | bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "121 | bert.encoder.layer.6.attention.output.dropout    | Dropout           | 0     \n",
      "122 | bert.encoder.layer.6.intermediate                | BertIntermediate  | 2 M   \n",
      "123 | bert.encoder.layer.6.intermediate.dense          | Linear            | 2 M   \n",
      "124 | bert.encoder.layer.6.output                      | BertOutput        | 2 M   \n",
      "125 | bert.encoder.layer.6.output.dense                | Linear            | 2 M   \n",
      "126 | bert.encoder.layer.6.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "127 | bert.encoder.layer.6.output.dropout              | Dropout           | 0     \n",
      "128 | bert.encoder.layer.7                             | BertLayer         | 7 M   \n",
      "129 | bert.encoder.layer.7.attention                   | BertAttention     | 2 M   \n",
      "130 | bert.encoder.layer.7.attention.self              | BertSelfAttention | 1 M   \n",
      "131 | bert.encoder.layer.7.attention.self.query        | Linear            | 590 K \n",
      "132 | bert.encoder.layer.7.attention.self.key          | Linear            | 590 K \n",
      "133 | bert.encoder.layer.7.attention.self.value        | Linear            | 590 K \n",
      "134 | bert.encoder.layer.7.attention.self.dropout      | Dropout           | 0     \n",
      "135 | bert.encoder.layer.7.attention.output            | BertSelfOutput    | 592 K \n",
      "136 | bert.encoder.layer.7.attention.output.dense      | Linear            | 590 K \n",
      "137 | bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "138 | bert.encoder.layer.7.attention.output.dropout    | Dropout           | 0     \n",
      "139 | bert.encoder.layer.7.intermediate                | BertIntermediate  | 2 M   \n",
      "140 | bert.encoder.layer.7.intermediate.dense          | Linear            | 2 M   \n",
      "141 | bert.encoder.layer.7.output                      | BertOutput        | 2 M   \n",
      "142 | bert.encoder.layer.7.output.dense                | Linear            | 2 M   \n",
      "143 | bert.encoder.layer.7.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "144 | bert.encoder.layer.7.output.dropout              | Dropout           | 0     \n",
      "145 | bert.encoder.layer.8                             | BertLayer         | 7 M   \n",
      "146 | bert.encoder.layer.8.attention                   | BertAttention     | 2 M   \n",
      "147 | bert.encoder.layer.8.attention.self              | BertSelfAttention | 1 M   \n",
      "148 | bert.encoder.layer.8.attention.self.query        | Linear            | 590 K \n",
      "149 | bert.encoder.layer.8.attention.self.key          | Linear            | 590 K \n",
      "150 | bert.encoder.layer.8.attention.self.value        | Linear            | 590 K \n",
      "151 | bert.encoder.layer.8.attention.self.dropout      | Dropout           | 0     \n",
      "152 | bert.encoder.layer.8.attention.output            | BertSelfOutput    | 592 K \n",
      "153 | bert.encoder.layer.8.attention.output.dense      | Linear            | 590 K \n",
      "154 | bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "155 | bert.encoder.layer.8.attention.output.dropout    | Dropout           | 0     \n",
      "156 | bert.encoder.layer.8.intermediate                | BertIntermediate  | 2 M   \n",
      "157 | bert.encoder.layer.8.intermediate.dense          | Linear            | 2 M   \n",
      "158 | bert.encoder.layer.8.output                      | BertOutput        | 2 M   \n",
      "159 | bert.encoder.layer.8.output.dense                | Linear            | 2 M   \n",
      "160 | bert.encoder.layer.8.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "161 | bert.encoder.layer.8.output.dropout              | Dropout           | 0     \n",
      "162 | bert.encoder.layer.9                             | BertLayer         | 7 M   \n",
      "163 | bert.encoder.layer.9.attention                   | BertAttention     | 2 M   \n",
      "164 | bert.encoder.layer.9.attention.self              | BertSelfAttention | 1 M   \n",
      "165 | bert.encoder.layer.9.attention.self.query        | Linear            | 590 K \n",
      "166 | bert.encoder.layer.9.attention.self.key          | Linear            | 590 K \n",
      "167 | bert.encoder.layer.9.attention.self.value        | Linear            | 590 K \n",
      "168 | bert.encoder.layer.9.attention.self.dropout      | Dropout           | 0     \n",
      "169 | bert.encoder.layer.9.attention.output            | BertSelfOutput    | 592 K \n",
      "170 | bert.encoder.layer.9.attention.output.dense      | Linear            | 590 K \n",
      "171 | bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "172 | bert.encoder.layer.9.attention.output.dropout    | Dropout           | 0     \n",
      "173 | bert.encoder.layer.9.intermediate                | BertIntermediate  | 2 M   \n",
      "174 | bert.encoder.layer.9.intermediate.dense          | Linear            | 2 M   \n",
      "175 | bert.encoder.layer.9.output                      | BertOutput        | 2 M   \n",
      "176 | bert.encoder.layer.9.output.dense                | Linear            | 2 M   \n",
      "177 | bert.encoder.layer.9.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "178 | bert.encoder.layer.9.output.dropout              | Dropout           | 0     \n",
      "179 | bert.encoder.layer.10                            | BertLayer         | 7 M   \n",
      "180 | bert.encoder.layer.10.attention                  | BertAttention     | 2 M   \n",
      "181 | bert.encoder.layer.10.attention.self             | BertSelfAttention | 1 M   \n",
      "182 | bert.encoder.layer.10.attention.self.query       | Linear            | 590 K \n",
      "183 | bert.encoder.layer.10.attention.self.key         | Linear            | 590 K \n",
      "184 | bert.encoder.layer.10.attention.self.value       | Linear            | 590 K \n",
      "185 | bert.encoder.layer.10.attention.self.dropout     | Dropout           | 0     \n",
      "186 | bert.encoder.layer.10.attention.output           | BertSelfOutput    | 592 K \n",
      "187 | bert.encoder.layer.10.attention.output.dense     | Linear            | 590 K \n",
      "188 | bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "189 | bert.encoder.layer.10.attention.output.dropout   | Dropout           | 0     \n",
      "190 | bert.encoder.layer.10.intermediate               | BertIntermediate  | 2 M   \n",
      "191 | bert.encoder.layer.10.intermediate.dense         | Linear            | 2 M   \n",
      "192 | bert.encoder.layer.10.output                     | BertOutput        | 2 M   \n",
      "193 | bert.encoder.layer.10.output.dense               | Linear            | 2 M   \n",
      "194 | bert.encoder.layer.10.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "195 | bert.encoder.layer.10.output.dropout             | Dropout           | 0     \n",
      "196 | bert.encoder.layer.11                            | BertLayer         | 7 M   \n",
      "197 | bert.encoder.layer.11.attention                  | BertAttention     | 2 M   \n",
      "198 | bert.encoder.layer.11.attention.self             | BertSelfAttention | 1 M   \n",
      "199 | bert.encoder.layer.11.attention.self.query       | Linear            | 590 K \n",
      "200 | bert.encoder.layer.11.attention.self.key         | Linear            | 590 K \n",
      "201 | bert.encoder.layer.11.attention.self.value       | Linear            | 590 K \n",
      "202 | bert.encoder.layer.11.attention.self.dropout     | Dropout           | 0     \n",
      "203 | bert.encoder.layer.11.attention.output           | BertSelfOutput    | 592 K \n",
      "204 | bert.encoder.layer.11.attention.output.dense     | Linear            | 590 K \n",
      "205 | bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "206 | bert.encoder.layer.11.attention.output.dropout   | Dropout           | 0     \n",
      "207 | bert.encoder.layer.11.intermediate               | BertIntermediate  | 2 M   \n",
      "208 | bert.encoder.layer.11.intermediate.dense         | Linear            | 2 M   \n",
      "209 | bert.encoder.layer.11.output                     | BertOutput        | 2 M   \n",
      "210 | bert.encoder.layer.11.output.dense               | Linear            | 2 M   \n",
      "211 | bert.encoder.layer.11.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "212 | bert.encoder.layer.11.output.dropout             | Dropout           | 0     \n",
      "213 | bert.pooler                                      | BertPooler        | 590 K \n",
      "214 | bert.pooler.dense                                | Linear            | 590 K \n",
      "215 | bert.pooler.activation                           | Tanh              | 0     \n",
      "216 | classifier                                       | Linear            | 769   \n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9060ff53da1494ebdb7ffe4f1d26ee2",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\r"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf98698817e94bb696192df37afd27e8",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RjEn6zr0q53B",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}