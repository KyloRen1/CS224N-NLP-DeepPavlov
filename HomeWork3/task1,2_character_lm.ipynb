{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2. Language modeling.\n",
    "\n",
    "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
    "\n",
    "\n",
    "\n",
    "## Task 1. Character-based language modeling: data preparation (15 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to en_UA.\n",
      "Warning: Failed to set locale category LC_TIME to en_UA.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_UA.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_UA.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_UA.\n",
      "--2020-04-01 19:24:17--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 533309 (521K) [text/plain]\n",
      "Saving to: ‘russian-train-high’\n",
      "\n",
      "russian-train-high  100%[===================>] 520.81K  1.86MB/s    in 0.3s    \n",
      "\n",
      "2020-04-01 19:24:17 (1.86 MB/s) - ‘russian-train-high’ saved [533309/533309]\n",
      "\n",
      "Warning: Failed to set locale category LC_NUMERIC to en_UA.\n",
      "Warning: Failed to set locale category LC_TIME to en_UA.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_UA.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_UA.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_UA.\n",
      "--2020-04-01 19:24:17--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 53671 (52K) [text/plain]\n",
      "Saving to: ‘russian-dev’\n",
      "\n",
      "russian-dev         100%[===================>]  52.41K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-04-01 19:24:18 (426 KB/s) - ‘russian-dev’ saved [53671/53671]\n",
      "\n",
      "Warning: Failed to set locale category LC_NUMERIC to en_UA.\n",
      "Warning: Failed to set locale category LC_TIME to en_UA.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_UA.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_UA.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_UA.\n",
      "--2020-04-01 19:24:19--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 53514 (52K) [text/plain]\n",
      "Saving to: ‘russian-test’\n",
      "\n",
      "russian-test        100%[===================>]  52.26K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-04-01 19:24:19 (409 KB/s) - ‘russian-test’ saved [53514/53514]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (1 points)**\n",
    "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_infile(infile):\n",
    "    lower = False\n",
    "    words = list()\n",
    "    with open(infile, newline = '') as games:                                                                                          \n",
    "        line = csv.reader(games, delimiter='\\t')\n",
    "        for word in line:\n",
    "            if ' ' not in word[1]:\n",
    "                if lower:\n",
    "                    words.append(word[1].lower())\n",
    "                else:\n",
    "                    words.append(word[1])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9213 917 922\n",
      "валлонскому незаконченным истрёпывав личного серьгам необоснованным тюти заросла идеальна гулкой\n"
     ]
    }
   ],
   "source": [
    "train_words = read_infile(\"russian-train-high\")\n",
    "dev_words = read_infile(\"russian-dev\")\n",
    "test_words = read_infile(\"russian-test\")\n",
    "print(len(train_words), len(dev_words), len(test_words))\n",
    "print(*train_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 19:24:21.470 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 49: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\"\"\"\n",
    "== YOUR CODE HERE ==\n",
    "\"\"\"        \n",
    "vocab = SimpleVocabulary(save_path='.', special_tokens=(\"END\", \"BEGIN\", \"UNK\", \"PAD\"))\n",
    "vocab.fit([list(x) for x in train_words])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 6, 23, 5, 15, 4, 8, 26, 6, 8, 8, 18, 14],\n",
       " ['н', 'е', 'з', 'а', 'к', 'о', 'н', 'ч', 'е', 'н', 'н', 'ы', 'м'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(list(train_words[1])), list(train_words[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "class Dataset(TorchDataset):\n",
    "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
    "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
    "        for the language model that obtain these word as input.        \n",
    "        \"\"\"\n",
    "        source_target = torch.LongTensor(self.vocab([\"BEGIN\"] + list(self.data[idx]) + [\"END\"]))\n",
    "        source = source_target[:-1]\n",
    "        target = source_target[1:]\n",
    "        return (source, target)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_words, vocab)\n",
    "dev_dataset = Dataset(dev_words, vocab)\n",
    "test_dataset = Dataset(test_words, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 12]) torch.int64 torch.Size([1, 12]) torch.int64\n",
      "1 torch.Size([1, 14]) torch.int64 torch.Size([1, 14]) torch.int64\n",
      "2 torch.Size([1, 11]) torch.int64 torch.Size([1, 11]) torch.int64\n",
      "3 torch.Size([1, 8]) torch.int64 torch.Size([1, 8]) torch.int64\n",
      "4 torch.Size([1, 8]) torch.int64 torch.Size([1, 8]) torch.int64\n",
      "5 torch.Size([1, 15]) torch.int64 torch.Size([1, 15]) torch.int64\n",
      "6 torch.Size([1, 5]) torch.int64 torch.Size([1, 5]) torch.int64\n",
      "7 torch.Size([1, 8]) torch.int64 torch.Size([1, 8]) torch.int64\n",
      "8 torch.Size([1, 9]) torch.int64 torch.Size([1, 9]) torch.int64\n",
      "9 torch.Size([1, 7]) torch.int64 torch.Size([1, 7]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batcher = iter(train_loader)\n",
    "for i in range(10):\n",
    "    (x, y) = next(batcher)\n",
    "    print(i, x.shape, x.dtype, y.shape, y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5) 1 point** Explain, why this does not work with larger batch size."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It doesn't work for batches of bigger size as all of the source and target vectors differe by the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def pad_tensor(vec, length, dim, pad_symbol):\n",
    "    \"\"\"\n",
    "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
    "    \"\"\"\n",
    "    pad_size = list(vec.shape)\n",
    "    pad_size[dim] = length - vec.size(dim)\n",
    "    return torch.cat((vec.type(torch.int64), torch.ones(*pad_size, dtype=torch.int64) * pad_symbol), dim=dim)\n",
    "\n",
    "class Padder:\n",
    "    def __init__(self, dim=0, pad_symbol=0): \n",
    "        self.dim = dim\n",
    "        self.pad_symbol = pad_symbol\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        max_length_x = max([len(x) for x,_ in batch]) \n",
    "        max_length_y = max([len(y) for _,y in batch])\n",
    "        x = torch.stack([pad_tensor(x, max_length_x, dim=self.dim, pad_symbol=self.pad_symbol) for x,_ in batch], 0) \n",
    "        y = torch.stack([pad_tensor(y, max_length_y, dim=self.dim, pad_symbol=self.pad_symbol) for _,y in batch], 0) \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "padder = Padder(dim=0, pad_symbol=vocab['PAD'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=Padder(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([8, 15]) torch.int64 torch.Size([8, 15]) torch.int64\n",
      "1 torch.Size([8, 15]) torch.int64 torch.Size([8, 15]) torch.int64\n",
      "2 torch.Size([8, 14]) torch.int64 torch.Size([8, 14]) torch.int64\n",
      "3 torch.Size([8, 16]) torch.int64 torch.Size([8, 16]) torch.int64\n",
      "4 torch.Size([8, 18]) torch.int64 torch.Size([8, 18]) torch.int64\n",
      "5 torch.Size([8, 16]) torch.int64 torch.Size([8, 16]) torch.int64\n",
      "6 torch.Size([8, 14]) torch.int64 torch.Size([8, 14]) torch.int64\n",
      "7 torch.Size([8, 11]) torch.int64 torch.Size([8, 11]) torch.int64\n",
      "8 torch.Size([8, 14]) torch.int64 torch.Size([8, 14]) torch.int64\n",
      "9 torch.Size([8, 11]) torch.int64 torch.Size([8, 11]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batcher = iter(train_loader)\n",
    "for i in range(10):\n",
    "    batch = next(batcher)\n",
    "    print(i, batch[0].shape, batch[0].dtype, batch[1].shape, batch[1].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Character-based language modeling. (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
    "1. **Embedding** layer that transforms input symbols into vectors.\n",
    "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
    "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_size):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dim)\n",
    "        self.rnn = nn.GRU(embeddings_dim, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        x = self.embeddings(inputs)\n",
    "        y, hidden = self.rnn(x, hidden)\n",
    "        logits = self.linear(y)\n",
    "        prob = self.softmax(logits)\n",
    "        return logits, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on_batch(model, criterion, x, y):\n",
    "    linear, prob = model(x)\n",
    "    loss = 0\n",
    "    for i in range(linear.shape[1]):\n",
    "        padding_mask = torch.tensor(y[:, i] != padder.pad_symbol, dtype=int)\n",
    "        loss += (criterion(linear[:, i, :], y[:, i]) * padding_mask).sum() / padding_mask.sum()\n",
    "    loss = loss / linear.shape[1]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, criterion, x, y, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    loss = validate_on_batch(model, criterion, x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
    "\n",
    "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 24\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "padder = Padder(dim=0, pad_symbol=vocab['PAD'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Padder(dim=0))\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Padder(dim=0))\n",
    "\n",
    "model = RNNLM(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 1.587787357935061, Validation Loss: 1.4095344512890546\n",
      "Epoch: 1, Training Loss: 1.357682776171714, Validation Loss: 1.3466459512710571\n",
      "Epoch: 2, Training Loss: 1.2991921721647184, Validation Loss: 1.3125305328613672\n",
      "Epoch: 3, Training Loss: 1.2572489061082404, Validation Loss: 1.2894700200129778\n",
      "Epoch: 4, Training Loss: 1.221893868719538, Validation Loss: 1.2735766172409058\n",
      "Epoch: 5, Training Loss: 1.190219511743635, Validation Loss: 1.2626772599342542\n",
      "Epoch: 6, Training Loss: 1.1609363177170355, Validation Loss: 1.2550962918843978\n",
      "Epoch: 7, Training Loss: 1.1333522057781618, Validation Loss: 1.250234270707155\n",
      "Epoch: 8, Training Loss: 1.107228999491781, Validation Loss: 1.2478399230883672\n",
      "Epoch: 9, Training Loss: 1.0825903185953696, Validation Loss: 1.2483548858226874\n"
     ]
    }
   ],
   "source": [
    "losses_train = list()\n",
    "losses_val = list()\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for i_batch, batch in enumerate(train_dataloader):\n",
    "        loss = train_on_batch(model, criterion, batch[0], batch[1], optimizer)\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    losses_train.append(train_loss)\n",
    "    \n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for i_batch, batch in enumerate(dev_dataloader):\n",
    "        with torch.no_grad():\n",
    "            loss = validate_on_batch(model, criterion, batch[0], batch[1])\n",
    "        val_loss += loss.item()\n",
    "    val_loss = val_loss / len(dev_dataloader)\n",
    "    losses_val.append(val_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses_train)\n",
    "plt.plot(losses_val)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_batch(model, batch):\n",
    "    with torch.no_grad():\n",
    "        linear, prob = model(batch[0])\n",
    "    probs = torch.ones(batch[1].shape)\n",
    "    for i in range(batch[1].shape[0]): \n",
    "        for j in range(batch[1].shape[1]):\n",
    "            probs[i, j] = prob[i, j, batch[1][i, j]]\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  пшеничной ; Letters probability:\n",
      "п 0.110\n",
      "ш 0.000\n",
      "е 0.454\n",
      "н 0.026\n",
      "и 0.054\n",
      "ч 0.044\n",
      "н 0.364\n",
      "о 0.281\n",
      "й 0.467\n",
      "============================================================\n",
      "Word:  переживаниями ; Letters probability:\n",
      "п 0.110\n",
      "е 0.089\n",
      "р 0.859\n",
      "е 0.856\n",
      "ж 0.015\n",
      "и 0.376\n",
      "в 0.277\n",
      "а 0.944\n",
      "н 0.294\n",
      "и 0.651\n",
      "я 0.634\n",
      "м 0.237\n",
      "и 0.268\n",
      "============================================================\n",
      "Word:  наибольшим ; Letters probability:\n",
      "н 0.066\n",
      "а 0.447\n",
      "и 0.023\n",
      "б 0.018\n",
      "о 0.052\n",
      "л 0.089\n",
      "ь 0.458\n",
      "ш 0.005\n",
      "и 0.453\n",
      "м 0.111\n",
      "============================================================\n",
      "Word:  правителями ; Letters probability:\n",
      "п 0.110\n",
      "р 0.408\n",
      "а 0.038\n",
      "в 0.544\n",
      "и 0.144\n",
      "т 0.036\n",
      "е 0.365\n",
      "л 0.717\n",
      "я 0.021\n",
      "м 0.459\n",
      "и 0.191\n",
      "============================================================\n",
      "Word:  апеллируешь ; Letters probability:\n",
      "а 0.048\n",
      "п 0.029\n",
      "е 0.046\n",
      "л 0.032\n",
      "л 0.005\n",
      "и 0.229\n",
      "р 0.104\n",
      "у 0.429\n",
      "е 0.138\n",
      "ш 0.153\n",
      "ь 0.974\n",
      "============================================================\n",
      "Word:  односложных ; Letters probability:\n",
      "о 0.084\n",
      "д 0.041\n",
      "н 0.341\n",
      "о 0.892\n",
      "с 0.153\n",
      "л 0.168\n",
      "о 0.088\n",
      "ж 0.273\n",
      "н 0.464\n",
      "ы 0.398\n",
      "х 0.231\n",
      "============================================================\n",
      "Word:  колодками ; Letters probability:\n",
      "к 0.051\n",
      "о 0.466\n",
      "л 0.092\n",
      "о 0.522\n",
      "д 0.088\n",
      "к 0.019\n",
      "а 0.182\n",
      "м 0.336\n",
      "и 0.511\n",
      "============================================================\n",
      "Word:  вибрирует ; Letters probability:\n",
      "в 0.080\n",
      "и 0.038\n",
      "б 0.004\n",
      "р 0.213\n",
      "и 0.522\n",
      "р 0.011\n",
      "у 0.338\n",
      "е 0.269\n",
      "т 0.300\n",
      "============================================================\n",
      "Word:  безразлично ; Letters probability:\n",
      "б 0.039\n",
      "е 0.290\n",
      "з 0.402\n",
      "р 0.045\n",
      "а 0.300\n",
      "з 0.277\n",
      "л 0.012\n",
      "и 0.303\n",
      "ч 0.285\n",
      "н 0.462\n",
      "о 0.381\n",
      "============================================================\n",
      "Word:  эпитрохоиду ; Letters probability:\n",
      "э 0.008\n",
      "п 0.018\n",
      "и 0.494\n",
      "т 0.347\n",
      "р 0.015\n",
      "о 0.222\n",
      "х 0.014\n",
      "о 0.234\n",
      "и 0.001\n",
      "д 0.140\n",
      "у 0.031\n",
      "============================================================\n",
      "Word:  общеобразовательного ; Letters probability:\n",
      "о 0.084\n",
      "б 0.315\n",
      "щ 0.065\n",
      "е 0.426\n",
      "о 0.001\n",
      "б 0.058\n",
      "р 0.508\n",
      "а 0.714\n",
      "з 0.358\n",
      "о 0.069\n",
      "в 0.635\n",
      "а 0.740\n",
      "т 0.059\n",
      "е 0.324\n",
      "л 0.968\n",
      "ь 0.589\n",
      "н 0.442\n",
      "о 0.322\n",
      "г 0.242\n",
      "о 0.995\n",
      "============================================================\n",
      "Word:  фригидной ; Letters probability:\n",
      "ф 0.011\n",
      "р 0.143\n",
      "и 0.179\n",
      "г 0.080\n",
      "и 0.112\n",
      "д 0.007\n",
      "н 0.352\n",
      "о 0.224\n",
      "й 0.466\n",
      "============================================================\n",
      "Word:  безмолвный ; Letters probability:\n",
      "б 0.039\n",
      "е 0.290\n",
      "з 0.402\n",
      "м 0.064\n",
      "о 0.257\n",
      "л 0.139\n",
      "в 0.004\n",
      "н 0.088\n",
      "ы 0.402\n",
      "й 0.044\n",
      "============================================================\n",
      "Word:  многолетним ; Letters probability:\n",
      "м 0.032\n",
      "н 0.060\n",
      "о 0.880\n",
      "г 0.711\n",
      "о 0.977\n",
      "л 0.050\n",
      "е 0.118\n",
      "т 0.178\n",
      "н 0.370\n",
      "и 0.206\n",
      "м 0.003\n",
      "============================================================\n",
      "Word:  оттопырьте ; Letters probability:\n",
      "о 0.084\n",
      "т 0.255\n",
      "т 0.050\n",
      "о 0.114\n",
      "п 0.195\n",
      "ы 0.025\n",
      "р 0.001\n",
      "ь 0.015\n",
      "т 0.020\n",
      "е 0.794\n",
      "============================================================\n",
      "Word:  долбануть ; Letters probability:\n",
      "д 0.043\n",
      "о 0.318\n",
      "л 0.105\n",
      "б 0.038\n",
      "а 0.387\n",
      "н 0.084\n",
      "у 0.036\n",
      "т 0.028\n",
      "ь 0.027\n",
      "============================================================\n",
      "Word:  синеватые ; Letters probability:\n",
      "с 0.127\n",
      "и 0.029\n",
      "н 0.497\n",
      "е 0.193\n",
      "в 0.079\n",
      "а 0.153\n",
      "т 0.053\n",
      "ы 0.203\n",
      "е 0.043\n",
      "============================================================\n",
      "Word:  колониальному ; Letters probability:\n",
      "к 0.051\n",
      "о 0.466\n",
      "л 0.092\n",
      "о 0.522\n",
      "н 0.105\n",
      "и 0.309\n",
      "а 0.002\n",
      "л 0.667\n",
      "ь 0.939\n",
      "н 0.947\n",
      "о 0.505\n",
      "м 0.180\n",
      "у 0.780\n",
      "============================================================\n",
      "Word:  надавливало ; Letters probability:\n",
      "н 0.066\n",
      "а 0.447\n",
      "д 0.082\n",
      "а 0.072\n",
      "в 0.161\n",
      "л 0.232\n",
      "и 0.614\n",
      "в 0.970\n",
      "а 0.890\n",
      "л 0.253\n",
      "о 0.261\n",
      "============================================================\n",
      "Word:  истерический ; Letters probability:\n",
      "и 0.031\n",
      "с 0.285\n",
      "т 0.325\n",
      "е 0.044\n",
      "р 0.271\n",
      "и 0.088\n",
      "ч 0.026\n",
      "е 0.167\n",
      "с 0.635\n",
      "к 0.982\n",
      "и 0.303\n",
      "й 0.125\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "test_prob = []\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Padder(dim=0))\n",
    "\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    prob = predict_on_batch(model, batch)\n",
    "    prob = prob.numpy().tolist()\n",
    "    test_prob += prob\n",
    "\n",
    "for i in range(-20, 0):\n",
    "    word = test_words[i]\n",
    "    print('Word: ', word, '; Letters probability:')\n",
    "    for j, letter in enumerate(word):\n",
    "        print(\"%s %.3f\" % (letter, test_prob[i][j]), end='\\n')\n",
    "    print('======'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, max_length=20, start_index=1, end_index=2):\n",
    "    model.eval()\n",
    "    x = torch.LongTensor([vocab(['BEGIN'])])\n",
    "    for i in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            linear, prob = model(x)\n",
    "        next_ = torch.multinomial(prob[0][-1], num_samples=1)\n",
    "        if next_.item() in vocab(['BEGIN', 'END', 'PAD', 'UNK']):\n",
    "            break\n",
    "        x = torch.cat([x[0], next_]).unsqueeze(0)\n",
    "    return x[0][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "илевлекты\n",
      "шут\n",
      "обельца\n",
      "короворционат\n",
      "допотеньким\n",
      "оторествам\n",
      "термину\n",
      "лесохианские\n",
      "выетдавший\n",
      "растрёрному\n",
      "трудозаистым\n",
      "поспет\n",
      "заёбрызный\n",
      "сгелями\n",
      "слогоционфасным\n",
      "сопымив\n",
      "осугильют\n",
      "струдительного\n",
      "изриста\n",
      "бубликам\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    word = ''.join([vocab[i] for i in generate(model).numpy().tolist()])\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
